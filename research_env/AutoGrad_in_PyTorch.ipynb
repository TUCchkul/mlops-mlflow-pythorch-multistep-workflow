{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3876cd2f",
   "metadata": {},
   "source": [
    "# Pytorch has a capability of automatic  gradient calculation !\n",
    "# In this Notebook We will learn each and every thing about autograd !\n",
    "\n",
    "#################### Auto Grad ######################################\n",
    "# Why we require auto grad !\n",
    "\"\"\" \n",
    "When we do backpropragation we need to calculate gradient of loss function w.r.t weigth \n",
    "If we do gradient calculation with hands it will take time and it wont be dynamic as then we would have to write  \n",
    "each derivative manually. To resolve this issue pytorch has a capability to calculate derivative of function automatically\n",
    "which is also known as Auto Grad.  \n",
    "\n",
    "\"\"\"\n",
    "import torch \n",
    "from torch.autograd import grad \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bb797",
   "metadata": {},
   "source": [
    "# A simplified model of a PyTorch tensor is as an object containing the following properties:\n",
    "1. data — a self-reference (per the above).\n",
    "2. required_grad — whether or not this tensor is/should be connected to the computational graph.\n",
    "3. grad — if required_grad is true, this prop will be a sub-tensor that collects the gradients against this tensor accumulated during backwards().\n",
    "4. grad_fn — This is a reference to the most recent operation which generated this tensor. PyTorch performs automatic differentiation by looking through the grad_fn list.\n",
    "5. is_leaf — Whether or not this is a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ebbe9",
   "metadata": {},
   "source": [
    "# Simple Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c78ccef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets Take an simple Example \n",
    "x=torch.tensor(5.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ae7849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10da32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets calculate Gradient by hand \n",
    "#  dy/dx = 2*x --- > 2x5 = 10\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a6dd0b",
   "metadata": {},
   "source": [
    "# Partial Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58691a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x00000198F8AD4908>\n",
      "tensor(10.)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# Now lets apply Partial derivative \n",
    "x = torch.tensor(5.0,requires_grad=True)\n",
    "y = torch.tensor(5.0,requires_grad=True)\n",
    "\n",
    "f = x**2 + y**2\n",
    "\n",
    "f.backward()\n",
    "# df/dx = 2*x --- > 2*5 =10\n",
    "# df/dy = 2*y --- > 2*5 =10\n",
    "\n",
    "print(f.grad_fn)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "# Here x , y Does not depend on each other they are in addition \n",
    "# So x has it's value independent of y and vice versa but \n",
    "# What if they are in multiply ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e37bb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeeb6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x00000198F8ACBE48>\n",
      "tensor(250.)\n",
      "tensor(250.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0,requires_grad=True)\n",
    "y = torch.tensor(5.0,requires_grad=True)\n",
    "\n",
    "f = x**2 * y**2\n",
    "\n",
    "f.backward()\n",
    "# df/dx = 2*x*y^2 --- > 2*5*25 = 250\n",
    "# df/dy = 2*y*x^2 --- > 2*5*25 = 250\n",
    "\n",
    "print(f.grad_fn)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc9f7e",
   "metadata": {},
   "source": [
    "# Nth derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7458b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads : 85.0\n",
      "Grads : 32.0\n",
      "Grads : 6.0\n",
      "Grads : 0.0\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Lets Do the double derivative\n",
    "def nth_derivative(f, wrt, n):\n",
    "\n",
    "    for i in range(n):\n",
    "        grads = grad(f, wrt, create_graph=True)[0]\n",
    "        print(f\"Grads : {grads}\")\n",
    "        f = grads.sum()\n",
    "        \n",
    "    return grads\n",
    "\n",
    "x = torch.tensor(5.0,requires_grad=True)\n",
    "\n",
    "f = x**2 + x**3\n",
    "print(nth_derivative(f=f, wrt=x, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554a498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(85., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nth_derivative(f=f, wrt=x, n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5310a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nth_derivative(f=f, wrt=x, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad7815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(nth_derivative(f=f, wrt=x, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670e2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
