{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f990f17d",
   "metadata": {},
   "source": [
    "# Pytorch has a capability of automatic  gradient calculation !\n",
    "# In this Notebook We will learn each and every thing about autograd !\n",
    "\n",
    "#################### Auto Grad ######################################\n",
    "# Why we require auto grad !\n",
    "\"\"\" \n",
    "When we do backpropragation we need to calculate gradient of loss function w.r.t weigth \n",
    "If we do gradient calculation with hands it will take time and it wont be dynamic as then we would have to write  \n",
    "each derivative manually. To resolve this issue pytorch has a capability to calculate derivative of function automatically\n",
    "which is also known as Auto Grad.  \n",
    "\n",
    "\"\"\"\n",
    "import torch \n",
    "from torch.autograd import grad \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bb797",
   "metadata": {},
   "source": [
    "# A simplified model of a PyTorch tensor is as an object containing the following properties:\n",
    "1. data — a self-reference (per the above).\n",
    "2. required_grad — whether or not this tensor is/should be connected to the computational graph.\n",
    "3. grad — if required_grad is true, this prop will be a sub-tensor that collects the gradients against this tensor accumulated during backwards().\n",
    "4. grad_fn — This is a reference to the most recent operation which generated this tensor. PyTorch performs automatic differentiation by looking through the grad_fn list.\n",
    "5. is_leaf — Whether or not this is a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ebbe9",
   "metadata": {},
   "source": [
    "# Simple Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c78ccef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets Take an simple Example \n",
    "x=torch.tensor(5.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ae7849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10da32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets calculate Gradient by hand \n",
    "#  dy/dx = 2*x --- > 2x5 = 10\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f7488",
   "metadata": {},
   "source": [
    "# Partial Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1dfb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x00000198F8AD4908>\n",
      "tensor(10.)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# Now lets apply Partial derivative \n",
    "x = torch.tensor(5.0,requires_grad=True)\n",
    "y = torch.tensor(5.0,requires_grad=True)\n",
    "\n",
    "f = x**2 + y**2\n",
    "\n",
    "f.backward()\n",
    "# df/dx = 2*x --- > 2*5 =10\n",
    "# df/dy = 2*y --- > 2*5 =10\n",
    "\n",
    "print(f.grad_fn)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "# Here x , y Does not depend on each other they are in addition \n",
    "# So x has it's value independent of y and vice versa but \n",
    "# What if they are in multiply ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569729e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f060faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x00000198F8ACBE48>\n",
      "tensor(250.)\n",
      "tensor(250.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0,requires_grad=True)\n",
    "y = torch.tensor(5.0,requires_grad=True)\n",
    "\n",
    "f = x**2 * y**2\n",
    "\n",
    "f.backward()\n",
    "# df/dx = 2*x*y^2 --- > 2*5*25 = 250\n",
    "# df/dy = 2*y*x^2 --- > 2*5*25 = 250\n",
    "\n",
    "print(f.grad_fn)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39096428",
   "metadata": {},
   "source": [
    "# Nth derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8dea982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads : 85.0\n",
      "Grad Sum : 85.0\n",
      "Grads : 32.0\n",
      "Grad Sum : 32.0\n",
      "Grads : 6.0\n",
      "Grad Sum : 6.0\n",
      "Grads : 0.0\n",
      "Grad Sum : 0.0\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Lets Do the double derivative\n",
    "def nth_derivative(f, wrt, n):\n",
    "\n",
    "    for i in range(n):\n",
    "        grads = grad(f, wrt, create_graph=True)[0]\n",
    "        print(f\"Grads : {grads}\")\n",
    "        f = grads.sum()\n",
    "        print(f\"Grad Sum : {f}\")\n",
    "        \n",
    "    return grads\n",
    "\n",
    "x = torch.tensor(5.0,requires_grad=True)\n",
    "\n",
    "f = x**2 + x**3\n",
    "print(nth_derivative(f=f, wrt=x, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d09b5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(85., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nth_derivative(f=f, wrt=x, n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9056eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nth_derivative(f=f, wrt=x, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5755106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(nth_derivative(f=f, wrt=x, n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4397d2",
   "metadata": {},
   "source": [
    "# Derivative of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19595a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 4., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derivative On A Tensor\n",
    "# This will give you error as gradient is only constructed for scaler values \n",
    "x = torch.tensor([5.0,4.0,3.0],requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c2b16ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25., 16.,  9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=x**2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b382a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11472\\2810694643.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIOPS\\mlops-mlflow-pythorch-multistep-workflow\\env\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIOPS\\mlops-mlflow-pythorch-multistep-workflow\\env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIOPS\\mlops-mlflow-pythorch-multistep-workflow\\env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f711619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 4., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to calculate gradient of tensor we need to convert them in scaler \n",
    "x = torch.tensor([5.0,4.0,3.0],requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e90ab6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25., 16.,  9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=x**2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e419524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7be081c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc083f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
